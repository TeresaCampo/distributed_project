{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f8ca82c",
   "metadata": {},
   "source": [
    "Execute if spirits are not visible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129eae11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import os\n",
    "\n",
    "current_path = Path.cwd()\n",
    "project_root = None\n",
    "for parent in [current_path] + list(current_path.parents):\n",
    "    if parent.name == \"distributed_project\":\n",
    "        project_root = parent\n",
    "        break\n",
    "os.chdir(project_root)\n",
    "print(f\"Set /distributed_project as working directory: {project_root}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dfae8a7",
   "metadata": {},
   "source": [
    "## Training with sparse reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c95e8e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/terra/anaconda3/envs/MARL/lib/python3.10/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from DQN import *\n",
    "from myenv_5_sparse_reward import MyGridWorld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e8155e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: ['agent1', 'agent2']\n",
      "Observation space: 7\n",
      "Action space: 5\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 10\n",
    "\n",
    "env = MyGridWorld(grid_size = GRID_SIZE)\n",
    "agents = {}\n",
    "action_size = env.action_space(\"agent1\").n\n",
    "obs_shape = env.observation_space(\"agent1\").shape[0]\n",
    "\n",
    "print(f\"Agents: {env.possible_agents}\\nObservation space: {obs_shape}\\nAction space: {action_size}\")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id] = DQNAgent(\n",
    "    obs_dim=obs_shape, \n",
    "    action_dim=action_size, \n",
    "    lr=0.001, \n",
    "    gamma=0.9,\n",
    "    epsilon=0.9\n",
    ")\n",
    "\n",
    "NUM_EPISODES = 5000\n",
    "EPSILON_DECAY_RATE = 0.9995\n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e61cd",
   "metadata": {},
   "source": [
    "Train or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a3fbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING \n",
    "trained_agents = train_agents(\n",
    "    env, \n",
    "    agents, \n",
    "    NUM_EPISODES, \n",
    "    EPSILON_DECAY_RATE, \n",
    "    MIN_EPSILON\n",
    ")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].save(f\"{agent_id}_sparse\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f430281",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].load(f\"{agent_id}_sparse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ad1412",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975a8131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "IQ TEST PHASE\n",
      "Num testing episodes: 100\n",
      "========================================\n",
      "Test Ep. 1/100: Status=TRUNCATED, Reward=-304.00, Steps=100\n",
      "Test Ep. 21/100: Status=TRUNCATED, Reward=-206.00, Steps=100\n",
      "Test Ep. 41/100: Status=TRUNCATED, Reward=-200.00, Steps=100\n",
      "Test Ep. 61/100: Status=TRUNCATED, Reward=-200.00, Steps=100\n",
      "Test Ep. 81/100: Status=TRUNCATED, Reward=-202.00, Steps=100\n",
      "END OF TESTING PHASE\n",
      "\n",
      "========================================\n",
      "TEST METRICS\n",
      "Success rate: 0.00%\n",
      "Avg reward per episode: -256.64\n",
      "Avg steps per episode: 100.00\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "env_test = MyGridWorld(grid_size = GRID_SIZE) \n",
    "test_agents(env_test, trained_agents, num_test_episodes=100)\n",
    "\n",
    "env.close()\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eae774c",
   "metadata": {},
   "source": [
    "## Dense reward, v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d30f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from myenv_5_dense_reward1 import MyGridWorld\n",
    "from DQN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb4e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agents: ['agent1', 'agent2']\n",
      "Observation space: 7\n",
      "Action space: 5\n",
      "========================================\n",
      "IQ TRAINING PHASE\n",
      "Num training episodes: 5000\n",
      "Espilon decaying rate: 0.9996\n",
      "========================================\n",
      "Episode 0/5000, Epsilon: 1.000, Episode joint_total_reward: -1005\n",
      "Episode 100/5000, Epsilon: 0.960, Episode joint_total_reward: -1226\n",
      "Episode 200/5000, Epsilon: 0.923, Episode joint_total_reward: -817\n",
      "Episode 300/5000, Epsilon: 0.887, Episode joint_total_reward: -1056\n",
      "Episode 400/5000, Epsilon: 0.852, Episode joint_total_reward: -1095\n",
      "Episode 500/5000, Epsilon: 0.818, Episode joint_total_reward: -618\n",
      "Episode 600/5000, Epsilon: 0.786, Episode joint_total_reward: -702\n",
      "Episode 700/5000, Epsilon: 0.755, Episode joint_total_reward: 4\n",
      "Episode 800/5000, Epsilon: 0.726, Episode joint_total_reward: -620\n",
      "Episode 900/5000, Epsilon: 0.697, Episode joint_total_reward: 21\n",
      "Episode 1000/5000, Epsilon: 0.670, Episode joint_total_reward: 22\n",
      "Episode 1100/5000, Epsilon: 0.644, Episode joint_total_reward: -497\n",
      "Episode 1200/5000, Epsilon: 0.618, Episode joint_total_reward: -32\n",
      "Episode 1300/5000, Epsilon: 0.594, Episode joint_total_reward: 51\n",
      "Episode 1400/5000, Epsilon: 0.571, Episode joint_total_reward: 141\n",
      "Episode 1500/5000, Epsilon: 0.549, Episode joint_total_reward: 162\n",
      "Episode 1600/5000, Epsilon: 0.527, Episode joint_total_reward: 155\n",
      "Episode 1700/5000, Epsilon: 0.506, Episode joint_total_reward: 74\n",
      "Episode 1800/5000, Epsilon: 0.486, Episode joint_total_reward: 191\n",
      "Episode 1900/5000, Epsilon: 0.467, Episode joint_total_reward: 17\n",
      "Episode 2000/5000, Epsilon: 0.449, Episode joint_total_reward: 170\n",
      "Episode 2100/5000, Epsilon: 0.431, Episode joint_total_reward: 135\n",
      "Episode 2200/5000, Epsilon: 0.415, Episode joint_total_reward: 120\n",
      "Episode 2300/5000, Epsilon: 0.398, Episode joint_total_reward: 108\n",
      "Episode 2400/5000, Epsilon: 0.383, Episode joint_total_reward: 187\n",
      "Episode 2500/5000, Epsilon: 0.368, Episode joint_total_reward: 154\n",
      "Episode 2600/5000, Epsilon: 0.353, Episode joint_total_reward: 180\n",
      "Episode 2700/5000, Epsilon: 0.339, Episode joint_total_reward: 179\n",
      "Episode 2800/5000, Epsilon: 0.326, Episode joint_total_reward: 118\n",
      "Episode 2900/5000, Epsilon: 0.313, Episode joint_total_reward: -274\n",
      "Episode 3000/5000, Epsilon: 0.301, Episode joint_total_reward: 145\n",
      "Episode 3100/5000, Epsilon: 0.289, Episode joint_total_reward: 192\n",
      "Episode 3200/5000, Epsilon: 0.278, Episode joint_total_reward: 180\n",
      "Episode 3300/5000, Epsilon: 0.267, Episode joint_total_reward: 150\n",
      "Episode 3400/5000, Epsilon: 0.256, Episode joint_total_reward: -244\n",
      "Episode 3500/5000, Epsilon: 0.246, Episode joint_total_reward: 192\n",
      "Episode 3600/5000, Epsilon: 0.237, Episode joint_total_reward: -383\n",
      "Episode 3700/5000, Epsilon: 0.227, Episode joint_total_reward: -266\n",
      "Episode 3800/5000, Epsilon: 0.219, Episode joint_total_reward: 156\n",
      "Episode 3900/5000, Epsilon: 0.210, Episode joint_total_reward: 182\n",
      "Episode 4000/5000, Epsilon: 0.202, Episode joint_total_reward: 186\n",
      "Episode 4100/5000, Epsilon: 0.194, Episode joint_total_reward: 173\n",
      "Episode 4200/5000, Epsilon: 0.186, Episode joint_total_reward: 176\n",
      "Episode 4300/5000, Epsilon: 0.179, Episode joint_total_reward: 173\n",
      "Episode 4400/5000, Epsilon: 0.172, Episode joint_total_reward: 186\n",
      "Episode 4500/5000, Epsilon: 0.165, Episode joint_total_reward: 114\n",
      "Episode 4600/5000, Epsilon: 0.159, Episode joint_total_reward: 173\n",
      "Episode 4700/5000, Epsilon: 0.152, Episode joint_total_reward: 164\n",
      "Episode 4800/5000, Epsilon: 0.146, Episode joint_total_reward: 159\n",
      "Episode 4900/5000, Epsilon: 0.141, Episode joint_total_reward: 165\n",
      "END OF IQ TRAINING PHASE\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 13\n",
    "\n",
    "env = MyGridWorld(grid_size = GRID_SIZE)\n",
    "agents = {}\n",
    "action_size = env.action_space(\"agent1\").n\n",
    "obs_shape = env.observation_space(\"agent1\").shape[0]\n",
    "\n",
    "print(f\"Agents: {env.possible_agents}\\nObservation space: {obs_shape}\\nAction space: {action_size}\")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id] = DQNAgent(\n",
    "    obs_dim=obs_shape, \n",
    "    action_dim=action_size, \n",
    "    lr=0.001, \n",
    "    gamma=0.9,\n",
    "    epsilon=1\n",
    ")\n",
    "\n",
    "NUM_EPISODES = 5000\n",
    "EPSILON_DECAY_RATE = 0.9996\n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd78a47f",
   "metadata": {},
   "source": [
    "Train or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806501aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING \n",
    "trained_agents = train_agents(\n",
    "    env, \n",
    "    agents, \n",
    "    NUM_EPISODES, \n",
    "    EPSILON_DECAY_RATE, \n",
    "    MIN_EPSILON\n",
    ")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].save(f\"{agent_id}_densev1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64465d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].load(f\"{agent_id}_densev1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1e392b",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2e9c1b60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "IQ TEST PHASE\n",
      "Num testing episodes: 100\n",
      "========================================\n",
      "WARNING! Sprite image ./sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/button.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/gate_open.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/gate_close.png not found. Using default sprite for this simulation.\n",
      "Test Ep. 1/100: Status=SUCCESS, Reward=179.00, Steps=8\n",
      "Test Ep. 21/100: Status=SUCCESS, Reward=185.00, Steps=9\n",
      "Test Ep. 41/100: Status=SUCCESS, Reward=198.00, Steps=3\n",
      "Test Ep. 61/100: Status=SUCCESS, Reward=158.00, Steps=11\n",
      "Test Ep. 81/100: Status=SUCCESS, Reward=166.00, Steps=10\n",
      "END OF TESTING PHASE\n",
      "\n",
      "========================================\n",
      "TEST METRICS\n",
      "Success rate: 100.00%\n",
      "Avg reward per episode: 180.38\n",
      "Avg steps per episode: 7.90\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "env_test = MyGridWorld(grid_size = GRID_SIZE, render_mode=\"human\") \n",
    "test_agents(env_test, trained_agents, num_test_episodes=100)\n",
    "\n",
    "env.close()\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd82e8a",
   "metadata": {},
   "source": [
    "## Dense reward, v2 with coherent reward function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6894c0a3",
   "metadata": {},
   "source": [
    "**Version with grid 13*13**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89216c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from myenv_5_dense_reward2 import MyGridWorld\n",
    "from DQN import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906adc83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agnets: ['agent1', 'agent2']\n",
      "Observation space: 7\n",
      "Action space: 5\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 13\n",
    "\n",
    "env = MyGridWorld(grid_size = GRID_SIZE)\n",
    "agents = {}\n",
    "action_size = env.action_space(\"agent1\").n\n",
    "obs_shape = env.observation_space(\"agent1\").shape[0]\n",
    "\n",
    "print(f\"Agnets: {env.possible_agents}\\nObservation space: {obs_shape}\\nAction space: {action_size}\")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id] = DQNAgent(\n",
    "    obs_dim=obs_shape, \n",
    "    action_dim=action_size, \n",
    "    lr=0.001, \n",
    "    gamma=0.9,\n",
    "    epsilon=1\n",
    ")\n",
    "\n",
    "NUM_EPISODES = 2500\n",
    "EPSILON_DECAY_RATE = 0.9996\n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c3a5a7",
   "metadata": {},
   "source": [
    "Train or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc64f755",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "IQ TRAINING PHASE\n",
      "Num training episodes: 2500\n",
      "Espilon decaying rate: 0.9996\n",
      "========================================\n",
      "Episode 0/2500, Epsilon: 1.000, Episode joint_total_reward: -63.30962558794381\n",
      "Episode 100/2500, Epsilon: 0.960, Episode joint_total_reward: -59.70669941596052\n",
      "Episode 200/2500, Epsilon: 0.923, Episode joint_total_reward: -40.4247505070384\n",
      "Episode 300/2500, Epsilon: 0.887, Episode joint_total_reward: -49.00839023734399\n",
      "Episode 400/2500, Epsilon: 0.852, Episode joint_total_reward: -41.72739739731466\n",
      "Episode 500/2500, Epsilon: 0.818, Episode joint_total_reward: 199.43042787807866\n",
      "Episode 600/2500, Epsilon: 0.786, Episode joint_total_reward: -35.08342325487675\n",
      "Episode 700/2500, Epsilon: 0.755, Episode joint_total_reward: -37.536405365534236\n",
      "Episode 800/2500, Epsilon: 0.726, Episode joint_total_reward: -19.96435480840395\n",
      "Episode 900/2500, Epsilon: 0.697, Episode joint_total_reward: -34.54499767178958\n",
      "Episode 1000/2500, Epsilon: 0.670, Episode joint_total_reward: -25.805962835460182\n",
      "Episode 1100/2500, Epsilon: 0.644, Episode joint_total_reward: -23.938025629926273\n",
      "Episode 1200/2500, Epsilon: 0.618, Episode joint_total_reward: -27.009216911391462\n",
      "Episode 1300/2500, Epsilon: 0.594, Episode joint_total_reward: -16.76841635301053\n",
      "Episode 1400/2500, Epsilon: 0.571, Episode joint_total_reward: -26.1307269123249\n",
      "Episode 1500/2500, Epsilon: 0.549, Episode joint_total_reward: -20.874107672366065\n",
      "Episode 1600/2500, Epsilon: 0.527, Episode joint_total_reward: -14.911207851182613\n",
      "Episode 1700/2500, Epsilon: 0.506, Episode joint_total_reward: -18.644089060824104\n",
      "Episode 1800/2500, Epsilon: 0.486, Episode joint_total_reward: -19.965891395669594\n",
      "Episode 1900/2500, Epsilon: 0.467, Episode joint_total_reward: -17.664300704522393\n",
      "Episode 2000/2500, Epsilon: 0.449, Episode joint_total_reward: -15.468583847558602\n",
      "Episode 2100/2500, Epsilon: 0.431, Episode joint_total_reward: -22.28259162653179\n",
      "Episode 2200/2500, Epsilon: 0.415, Episode joint_total_reward: -13.12423005175582\n",
      "Episode 2300/2500, Epsilon: 0.398, Episode joint_total_reward: -15.54509093893378\n",
      "Episode 2400/2500, Epsilon: 0.383, Episode joint_total_reward: 196.3153933522709\n",
      "END OF IQ TRAINING PHASE\n",
      "Model saved in agent1_densev2_2.pt\n",
      "Model saved in agent2_densev2_2.pt\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "trained_agents = train_agents(\n",
    "    env, \n",
    "    agents, \n",
    "    NUM_EPISODES, \n",
    "    EPSILON_DECAY_RATE, \n",
    "    MIN_EPSILON\n",
    ")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].save(f\"{agent_id}_densev2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6d648",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].load(f\"{agent_id}_densev2_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14288a4",
   "metadata": {},
   "source": [
    "Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038e351b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "IQ TEST PHASE\n",
      "Num testing episodes: 100\n",
      "========================================\n",
      "WARNING! Sprite image ./sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/button.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/gate_open.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/gate_close.png not found. Using default sprite for this simulation.\n",
      "Test Ep. 1/100: Status=SUCCESS, Reward=199.02, Steps=5\n",
      "Test Ep. 21/100: Status=SUCCESS, Reward=198.88, Steps=10\n",
      "Test Ep. 41/100: Status=SUCCESS, Reward=196.93, Steps=10\n",
      "Test Ep. 61/100: Status=SUCCESS, Reward=197.19, Steps=13\n",
      "Test Ep. 81/100: Status=SUCCESS, Reward=197.03, Steps=11\n",
      "END OF TESTING PHASE\n",
      "\n",
      "========================================\n",
      "TEST METRICS\n",
      "Success rate: 100.00%\n",
      "Avg reward per episode: 198.62\n",
      "Avg steps per episode: 8.40\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "env_test = MyGridWorld(grid_size = GRID_SIZE, render_mode=\"human\") \n",
    "test_agents(env_test, trained_agents, num_test_episodes=100)\n",
    "\n",
    "env.close()\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff70bf1",
   "metadata": {},
   "source": [
    "**Version with grid 15*15**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d084c670",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agnets: ['agent1', 'agent2']\n",
      "Observation space: 7\n",
      "Action space: 5\n"
     ]
    }
   ],
   "source": [
    "GRID_SIZE = 15\n",
    "\n",
    "env = MyGridWorld(grid_size = GRID_SIZE)\n",
    "agents = {}\n",
    "action_size = env.action_space(\"agent1\").n\n",
    "obs_shape = env.observation_space(\"agent1\").shape[0]\n",
    "\n",
    "print(f\"Agnets: {env.possible_agents}\\nObservation space: {obs_shape}\\nAction space: {action_size}\")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id] = DQNAgent(\n",
    "    obs_dim=obs_shape, \n",
    "    action_dim=action_size, \n",
    "    lr=0.001, \n",
    "    gamma=0.9,\n",
    "    epsilon=1\n",
    ")\n",
    "\n",
    "NUM_EPISODES = 5000\n",
    "EPSILON_DECAY_RATE = 0.9997\n",
    "MIN_EPSILON = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5e594",
   "metadata": {},
   "source": [
    "Train or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e342d017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "IQ TRAINING PHASE\n",
      "Num training episodes: 5000\n",
      "Espilon decaying rate: 0.9997\n",
      "========================================\n",
      "Episode 0/5000, Epsilon: 1.000, Episode joint_total_reward: -86.06103659210683\n",
      "Episode 100/5000, Epsilon: 0.970, Episode joint_total_reward: -60.30122087323233\n",
      "Episode 200/5000, Epsilon: 0.941, Episode joint_total_reward: -64.67133872604983\n",
      "Episode 300/5000, Epsilon: 0.914, Episode joint_total_reward: -45.581371742268395\n",
      "Episode 400/5000, Epsilon: 0.887, Episode joint_total_reward: -38.31011504814913\n",
      "Episode 500/5000, Epsilon: 0.860, Episode joint_total_reward: -45.43268121644287\n",
      "Episode 600/5000, Epsilon: 0.835, Episode joint_total_reward: -37.43825153838172\n",
      "Episode 700/5000, Epsilon: 0.810, Episode joint_total_reward: -49.182498901777535\n",
      "Episode 800/5000, Epsilon: 0.786, Episode joint_total_reward: -35.3108054410063\n",
      "Episode 900/5000, Epsilon: 0.763, Episode joint_total_reward: -51.55158768297595\n",
      "Episode 1000/5000, Epsilon: 0.741, Episode joint_total_reward: 191.01129968299873\n",
      "Episode 1100/5000, Epsilon: 0.719, Episode joint_total_reward: 170.38869699422796\n",
      "Episode 1200/5000, Epsilon: 0.697, Episode joint_total_reward: 168.4427368058945\n",
      "Episode 1300/5000, Epsilon: 0.677, Episode joint_total_reward: 176.74777897602542\n",
      "Episode 1400/5000, Epsilon: 0.657, Episode joint_total_reward: 190.06364183979278\n",
      "Episode 1500/5000, Epsilon: 0.637, Episode joint_total_reward: 184.29321902191228\n",
      "Episode 1600/5000, Epsilon: 0.619, Episode joint_total_reward: 191.62870906190133\n",
      "Episode 1700/5000, Epsilon: 0.600, Episode joint_total_reward: 180.04986477621327\n",
      "Episode 1800/5000, Epsilon: 0.583, Episode joint_total_reward: 179.61744092188343\n",
      "Episode 1900/5000, Epsilon: 0.565, Episode joint_total_reward: 186.3803776484132\n",
      "Episode 2000/5000, Epsilon: 0.549, Episode joint_total_reward: 193.26386703552558\n",
      "Episode 2100/5000, Epsilon: 0.532, Episode joint_total_reward: 194.0975771360152\n",
      "Episode 2200/5000, Epsilon: 0.517, Episode joint_total_reward: 195.65721534502265\n",
      "Episode 2300/5000, Epsilon: 0.501, Episode joint_total_reward: 196.40909558836773\n",
      "Episode 2400/5000, Epsilon: 0.487, Episode joint_total_reward: 196.27485150442823\n",
      "Episode 2500/5000, Epsilon: 0.472, Episode joint_total_reward: 194.28147196027953\n",
      "Episode 2600/5000, Epsilon: 0.458, Episode joint_total_reward: 194.31887874995633\n",
      "Episode 2700/5000, Epsilon: 0.445, Episode joint_total_reward: 192.00924449536694\n",
      "Episode 2800/5000, Epsilon: 0.432, Episode joint_total_reward: 197.92501184678085\n",
      "Episode 2900/5000, Epsilon: 0.419, Episode joint_total_reward: 187.6859587650011\n",
      "Episode 3000/5000, Epsilon: 0.406, Episode joint_total_reward: 193.80789088568218\n",
      "Episode 3100/5000, Epsilon: 0.394, Episode joint_total_reward: 193.28952338646624\n",
      "Episode 3200/5000, Epsilon: 0.383, Episode joint_total_reward: 194.74966280274808\n",
      "Episode 3300/5000, Epsilon: 0.371, Episode joint_total_reward: 196.99739265650376\n",
      "Episode 3400/5000, Epsilon: 0.360, Episode joint_total_reward: 194.8731097459319\n",
      "Episode 3500/5000, Epsilon: 0.350, Episode joint_total_reward: 197.85276764349524\n",
      "Episode 3600/5000, Epsilon: 0.339, Episode joint_total_reward: 198.0495263683717\n",
      "Episode 3700/5000, Epsilon: 0.329, Episode joint_total_reward: 197.90624551762795\n",
      "Episode 3800/5000, Epsilon: 0.320, Episode joint_total_reward: 195.99894246894252\n",
      "Episode 3900/5000, Epsilon: 0.310, Episode joint_total_reward: 197.133123385877\n",
      "Episode 4000/5000, Epsilon: 0.301, Episode joint_total_reward: 196.29903409277253\n",
      "Episode 4100/5000, Epsilon: 0.292, Episode joint_total_reward: 195.48656213336136\n",
      "Episode 4200/5000, Epsilon: 0.284, Episode joint_total_reward: 198.20818095716163\n",
      "Episode 4300/5000, Epsilon: 0.275, Episode joint_total_reward: 197.89668801730625\n",
      "Episode 4400/5000, Epsilon: 0.267, Episode joint_total_reward: 197.27666267337926\n",
      "Episode 4500/5000, Epsilon: 0.259, Episode joint_total_reward: 195.56937138147157\n",
      "Episode 4600/5000, Epsilon: 0.251, Episode joint_total_reward: 198.89264395614475\n",
      "Episode 4700/5000, Epsilon: 0.244, Episode joint_total_reward: 195.6471202104335\n",
      "Episode 4800/5000, Epsilon: 0.237, Episode joint_total_reward: 190.34796705300838\n",
      "Episode 4900/5000, Epsilon: 0.230, Episode joint_total_reward: 196.60801673959682\n",
      "END OF IQ TRAINING PHASE\n",
      "Model saved in agent1_densev2_1515_1.pt\n",
      "Model saved in agent2_densev2_1515_1.pt\n"
     ]
    }
   ],
   "source": [
    "# TRAINING \n",
    "trained_agents = train_agents(\n",
    "    env, \n",
    "    agents, \n",
    "    NUM_EPISODES, \n",
    "    EPSILON_DECAY_RATE, \n",
    "    MIN_EPSILON\n",
    ")\n",
    "\n",
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].save(f\"{agent_id}_densev2_1515\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ae17f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for agent_id in env.possible_agents:\n",
    "    agents[agent_id].load(f\"{agent_id}_densev2_1515_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "42aaf573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========================================\n",
      "IQ TEST PHASE\n",
      "Num testing episodes: 100\n",
      "========================================\n",
      "WARNING! Sprite image ./sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/button.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/gate_open.png not found. Using default sprite for this simulation.\n",
      "WARNING! Sprite image ./sprites/gate_close.png not found. Using default sprite for this simulation.\n",
      "Test Ep. 1/100: Status=SUCCESS, Reward=197.78, Steps=12\n",
      "Test Ep. 21/100: Status=SUCCESS, Reward=195.76, Steps=13\n",
      "Test Ep. 41/100: Status=SUCCESS, Reward=198.06, Steps=7\n",
      "Test Ep. 61/100: Status=SUCCESS, Reward=197.23, Steps=10\n",
      "Test Ep. 81/100: Status=SUCCESS, Reward=197.70, Steps=8\n",
      "END OF TESTING PHASE\n",
      "\n",
      "========================================\n",
      "TEST METRICS\n",
      "Success rate: 100.00%\n",
      "Avg reward per episode: 197.30\n",
      "Avg steps per episode: 10.34\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "env_test = MyGridWorld(grid_size = GRID_SIZE, render_mode=\"human\") \n",
    "test_agents(env_test, trained_agents, num_test_episodes=100)\n",
    "\n",
    "env.close()\n",
    "env_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d407ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MARL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
